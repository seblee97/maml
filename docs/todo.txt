Short-term Todos
- plotting of priority queue scaling to parameters
- check 'extra input biases' used by finn et al (referenced by cavia paper)
- probably should check batch size consistency of naming, particularly for trianing vs validating. What is K, what is number of gradient updates, what is number of tasks being sampled.
- write some tests:
        updating of priority queue
        make sure regression converges when lots of updates/k high
- clean up jax, add documentation and ensure similar interfaces to pytorch model. Merge and make train different for jax/pytorch?
- clean up results folder
- do runs on jax, pytorch model

Long-term Todos
- Image completion regression task
- RL maml then RL maml plus pq
- Investigate cavia + pq 
- Alternatives to vanilla pq (potential msc investigation)
- Can the network used be the standard one, speak to jarek to reengineer for efficiency?

Thoughts
- if sampling more from higher loss regions of the parameter space leads to a generally less effective meta prior, 
  in that it pushes the meta parameter to a point where it no longer initialises well for the bulk of the 'easier' tasks in the distributions, 
  how do you ensure that there are no systematic tails in the performance of the meta parameter?
- Links to fairness
- Does current loss metric accurately captures varying success of meta params for tasks in distribution

Done
- model checkpointing doesn't seem to work (fixed maybe - test)
- random seeds
- plot/log variance of meta validation loss
- figure out how to structure repos (folder for experiment scrvipts, folder for model scripts etc.)
- compute correlation between how often a parameter range has been picked and the loss in that grid
- Make repos a package - with setup for dev
- Change paths to be consistent with the new package structure
- pdf of losses (to see range/ skew etc)
- plot sum of priority queues (ensure documentation of clear distinction between this and rthe meta validation loss being computed)
- change correlation plot to spearmans or something more interpretable between -1 and 1
- add interpolate etc. methods for priority queue query 
- make a base config and then a parser of sorts for setting separate things, will ensure more consistency in hyperparameters
- amplitude 5, (finn), amplitude 0.5 (zintgraf), learning rate 0.01 (finn)
- run experiments again with amplitudes = 0.1 = 0.5 (check finn et al for parameters used)
- make dev and exp branch